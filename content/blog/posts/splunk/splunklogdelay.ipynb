{
 "cells": [
  {
   "attachments": {},
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Log Timing Delays in Splunk Could be Putting Your Security at Risk\n",
    "description: \"Discover how minor logging delays can lead to missed Splunk alerts and put your security at risk. Learn how to avoid this issue.\"\n",
    "author: Michael McKinley\n",
    "date: '2023-04-19'\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "        code-tools: true\n",
    "        page-layout: full\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLUF:** Logging delay to Splunk as little as 100ms can cause missed alerts.\n",
    "\n",
    "### A Tale of Two Times\n",
    "\n",
    "Splunk is a powerful platform for collecting, indexing and analyzing data from various sources. However, it is critically important to understand the nuances of how Splunk handles time; forgoing such consideration may lead to missing critical events. \n",
    "\n",
    "#### _time\n",
    "The `_time` field in Splunk represents the timestamp of an event as specified by the source log. This field is critical for time-based analysis, such as identifying trends, detecting anomalies, and correlating events across different sources. This is the time that Splunk visualizes when viewing search results and is used as the time variable when performing searches.\n",
    "\n",
    "#### _indextime\n",
    "The `_indextime` field in Splunk represents the timestamp when Splunk indexes the event, which may not always be the same as the `_time` field. Simply, it's the time Splunk logged the event, and it will almost always differ from `_time` even if by milliseconds. \n",
    "\n",
    "### A Wrinkle in Time\n",
    "\n",
    "**Why do the times differ?** \n",
    "\n",
    "Truth is, there is a delay. There are many points in the ingestion pipeline that can cause delay, such as:\n",
    "\n",
    "* Network latency\n",
    "* Logging throughput capacity\n",
    "* Logging interval \n",
    "* Clock skew\n",
    "\n",
    "Timing delays in Splunk can cause missed alerts, which may result in serious consequences for security and other critical operations. \n",
    "\n",
    "For instance, if an alert is monitoring a critical system, but there is a delay between the `_time` and `_indextime` timestamps, **the alert may not be triggered even when events occur.**\n",
    "\n",
    "### Small Delay = Big Problem\n",
    "\n",
    "**How much delay can cause missed events?**\n",
    "\n",
    "Delays of milliseconds can result in missed events. During testing in the worst case scenario, **delays of 100ms resulted in missed events**. Theoretically the delay can be even be lower. This raises an important question - are you confident that all of your critical logging sources consistently log in near real-time?\n",
    "\n",
    "### Delay Detection\n",
    "\n",
    "**Can I calculate delays in my log sources?**\n",
    "\n",
    "Yes, Splunk has a great article on this: [here](https://docs.splunk.com/Documentation/Splunk/9.0.4/Troubleshooting/Troubleshootingeventsindexingdelay). \n",
    "\n",
    "TL;DR To discover log delays subtract `_time` from `_indextime`.\n",
    "\n",
    "`index=example_delay | eval delay_sec=_indextime-_time | eval delay_minute=(delay_sec/60) | timechart span=1h avg(delay_minute) by index`\n",
    "\n",
    "![](splunk_blog_1.png \"Fig 1. Example Timechart for Splunk Search\")\n",
    "_Fig 1. Example Timechart for Splunk Search_\n",
    "\n",
    "**Can I discover if any of my recent saved searches missed events?**\n",
    "\n",
    "Yes, a simple method is to re-run the search over the same period and compare the results. If the results differ over there is likely a timing issue. \n",
    "\n",
    "Below is an example script in Python to identify saved searches with timing issues. Usage information: [here](https://github.com/SILJAEUROPA/Splunk-Delay-Detector).\n",
    "\n",
    "I've run this script against a lab instance of Splunk with intentionally induced delays, the results are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to detect and graph differences in recently ran searches\n",
    "\n",
    "# Import required libraries\n",
    "import time\n",
    "import json\n",
    "import splunklib.client as client\n",
    "import splunklib.results as results\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "# Function to connect to the Splunk service\n",
    "def splunk_connect(base_url, port, splunk_account, splunk_password):\n",
    "\n",
    "    try:\n",
    "        # Connect to the Splunk service\n",
    "        service = client.connect(host=base_url, port=port, username=splunk_account, password=splunk_password)\n",
    "        print('Connected to Splunk instance: {}'.format(base_url))\n",
    "        print('-' * 90)\n",
    "    except Exception as e:\n",
    "        print('Error in Connecting to Splunk: {}'.format(e))\n",
    "    \n",
    "    # Return the service object\n",
    "    return service\n",
    "\n",
    "\n",
    "# Function to compare jobs\n",
    "def compare_jobs(service,owner,graph):\n",
    "    \n",
    "    # Get the list of search jobs\n",
    "    search_jobs = service.jobs.list()\n",
    "    \n",
    "    # Loop through the search jobs\n",
    "    for job in search_jobs:\n",
    "        # If the job is a saved search and the owner matches the specified owner\n",
    "        if job['content']['isSavedSearch'] == '1' and job['access']['owner'] == owner:\n",
    "     \n",
    "            # Initialize lists to store results\n",
    "            results_list = []\n",
    "            results_list_new = []\n",
    "            \n",
    "            # Set the keyword arguments for the search job\n",
    "            kwargs =    {\"earliest_time\": job['content']['earliestTime'],\n",
    "                         \"latest_time\": job['content']['latestTime'],\n",
    "                         \"search_mode\": \"normal\",\n",
    "                         \"output_mode\": \"json\"}\n",
    "                \n",
    "            # Get the results of the original job\n",
    "            job_results = job.results(output_mode='json')\n",
    "            \n",
    "            # Parse the results and append them to the results list\n",
    "            for x in results.JSONResultsReader(job_results):\n",
    "                results_list.append(dict(x))\n",
    "                    \n",
    "            # Create a new job with the same search criteria\n",
    "            job_new = service.jobs.create(job['content']['eventSearch'],**kwargs)\n",
    "            \n",
    "            # Wait for the new job to finish\n",
    "            while not job_new.is_done():\n",
    "                time.sleep(1)\n",
    "                    \n",
    "            # Get the results of the new job\n",
    "            job_results_new = job_new.results(output_mode='json')\n",
    "            \n",
    "            # Parse the results and append them to the new results list\n",
    "            for y in results.JSONResultsReader(job_results_new):\n",
    "                results_list_new.append(dict(y))\n",
    "                \n",
    "            # Compare the lengths of the results lists\n",
    "            if(len(results_list) != len(results_list_new)):\n",
    "                # Initialize lists to store issues\n",
    "                search_issue = []\n",
    "                results_indexed = []\n",
    "                results_time = []\n",
    "                # Add the search issue to the list\n",
    "                search_issue.append({\n",
    "                    'sid': job['content']['sid'],\n",
    "                    'label': job['content']['label'],\n",
    "                    'earliest_time': job['content']['earliestTime'],\n",
    "                    'latest_time': job['content']['latestTime'],\n",
    "                })\n",
    "                    \n",
    "                # Get the unique keys for the results\n",
    "                results_keys = [(d.get('_raw'), d.get('_indextime'), d.get('_time')) for d in results_list]\n",
    "                results_keys_new = [(d.get('_raw'), d.get('_indextime'), d.get('_time')) for d in results_list_new]\n",
    "                \n",
    "                # Find the unique results in the new results list\n",
    "                unique_to_new = set(results_keys_new) - set(results_keys)\n",
    "\n",
    "                # print the unique results\n",
    "                if unique_to_new:\n",
    "                    print('SEARCH: {} SID: {}'.format(job['content']['label'],job['content']['sid']))\n",
    "                    print('RESULT: DIFFERENCE FOUND')\n",
    "                    print('NOTE: This is indicative of a delay issue and resulted in missing events in the original search.')\n",
    "                    print('-' * 10)\n",
    "                    print('List of Missing Results')\n",
    "                    print('-' * 10)\n",
    "                    \n",
    "                    #format time and set in index\n",
    "                    for key in unique_to_new:\n",
    "                        formatted_indextime = datetime.utcfromtimestamp(int(key[1])).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                        results_indexed.append(formatted_indextime)\n",
    "                        formatted_time = datetime.fromisoformat(key[2]).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                        results_time.append(formatted_time)\n",
    "                        print('        _time={},_indextime={},_raw={}'.format(formatted_time,formatted_indextime,key[0]))\n",
    "                    \n",
    "                    #graph the results    \n",
    "                    if graph == 'true':     \n",
    "                        plot_timeline(search_issue,results_indexed,results_time)\n",
    "                    print('-' * 90)\n",
    "            else:\n",
    "                print('SEARCH: {} SID: {}'.format(job['content']['label'],job['content']['sid']))\n",
    "                print('RESULT: NO DELAY FOUND')\n",
    "                print('NOTE: Search results in original search and re-ran search match, no delay issue identified.')\n",
    "                print('-' * 90)\n",
    " \n",
    "\n",
    "# Function to plot the timeline of the search issue\n",
    "def plot_timeline(search_issue, results_indexed, results_time):\n",
    "    \n",
    "    # Convert result issue timestamps to datetime objects\n",
    "    indextimes = [datetime.strptime(d, '%Y-%m-%d %H:%M:%S') for d in results_indexed]\n",
    "    times = [datetime.strptime(d, '%Y-%m-%d %H:%M:%S') for d in results_time]\n",
    "    # Set the title and subtitle for the plot\n",
    "    title = search_issue[0]['label']\n",
    "    subtitle = search_issue[0]['sid']\n",
    "    \n",
    "    # Get the start and end dates of the search issue and remove timezone and microseconds\n",
    "    start_date = datetime.strptime(search_issue[0]['earliest_time'], '%Y-%m-%dT%H:%M:%S.%f%z')\n",
    "    start_date = start_date.replace(microsecond=0, tzinfo=None)\n",
    "    end_date = datetime.strptime(search_issue[0]['latest_time'], '%Y-%m-%dT%H:%M:%S.%f%z')\n",
    "    end_date = end_date.replace(microsecond=0, tzinfo=None)\n",
    "\n",
    "    # Create a figure and axis for the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot the base timeline\n",
    "    ax.plot([start_date, end_date], [0, 0], color='black', alpha=0.5)\n",
    "    # Fill the area between the start and end dates\n",
    "    ax.fill_between([start_date, end_date], -0.5, 0.5, color='green', alpha=0.2)\n",
    "\n",
    "    # Calculate the maximum difference between indextime and start_time\n",
    "    max_difference = max([abs((date - start_date).total_seconds()) for date in indextimes] +\n",
    "                         [abs((date - end_date).total_seconds()) for date in indextimes])\n",
    "    \n",
    "    # Set the buffer based on the maximum difference\n",
    "    buffer = timedelta(seconds=max_difference * 0.1)\n",
    "\n",
    "    # Set the x and y limits for the plot\n",
    "    ax.set_xlim(start_date - buffer, end_date + buffer)\n",
    "    ax.set_ylim(-0.5, 0.5)\n",
    "\n",
    "    # Remove spines and y-axis\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.yaxis.set_visible(False)\n",
    "\n",
    "    # Plot the indexed markers and annotations\n",
    "    for idx, date in enumerate(indextimes):\n",
    "        ax.plot(date, 0, 'ro', markersize=8, label='Indexed Time for Event')\n",
    "        ax.annotate('{} - {}'.format(idx,date.strftime('%m-%d-%Y %H:%M:%S')), xy=(date, 0), xytext=(date, 0.1 + (idx % 2) * 0.1),\n",
    "                arrowprops=dict(arrowstyle='->', lw=1.5),\n",
    "                fontsize=12, ha='center')\n",
    "        \n",
    "    #plot the time markers and annotations\n",
    "    for idx, date in enumerate(times):\n",
    "        ax.plot(date, 0, 'bo', markersize=8, label='Time for Event')\n",
    "        ax.annotate('{} - {}'.format(idx,date.strftime('%m-%d-%Y %H:%M:%S')), xy=(date, 0), xytext=(date, -0.1 + (idx % 2) * -0.1),\n",
    "                arrowprops=dict(arrowstyle='->', lw=1.5),\n",
    "                fontsize=12, ha='center')\n",
    "\n",
    "    # Set the title, x-axis formatter, and x-axis locator\n",
    "    ax.set_title('{}\\n{}'.format(title,subtitle), fontsize=16, fontweight='bold')\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d-%Y %H:%M:%S'))\n",
    "    ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "\n",
    "    # Create the legend and display unique items\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax.legend(by_label.values(), by_label.keys(), loc='upper left')\n",
    "    \n",
    "    # Adjust layout and display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def banner():\n",
    "    print('-' * 90)\n",
    "    print('-' * 90)\n",
    "    print(' _____  _____       _____  _   _   _____  _____ _____ _____ _____ _____ _____ _    _ _____')\n",
    "    print(' |    \\ |____ |     |___|   \\_/    |    \\ |____   |   |____ |       |     |    \\  /  |____')\n",
    "    print(' |____/ |____ |____ |   |    |     |____/ |____   |   |____ |_____  |   __|__   \\/   |____')\n",
    "    print('                                 Splunk Log Delay Detector                            v1.0')\n",
    "    print('-' * 90)\n",
    "    print('-' * 90)\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    \n",
    "    #get config\n",
    "    with open('config.json') as f:\n",
    "        data = json.load(f)\n",
    "        splunk_account = data['splunk_account']\n",
    "        splunk_password = data ['splunk_password']\n",
    "        base_url = data['base_url']\n",
    "        port = data['port']\n",
    "        owner = data['owner']\n",
    "        graph = data['graph']\n",
    "  \n",
    "    #show banner\n",
    "    banner()\n",
    "  \n",
    "    #connect to splunk\n",
    "    service = splunk_connect(base_url, port, splunk_account, splunk_password)\n",
    "    \n",
    "    #check and compare all jobs by owner\n",
    "    compare_jobs(service,owner,graph) \n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Solution\n",
    "\n",
    "Rather than telling Splunk to alert to items within the last `_time` window for our searches, we can alert to anything indexed in the period since our search last ran. Instead saying, \"Splunk tell me about any log you just discovered.\" \n",
    "\n",
    "To do this the alert requires two modifications:\n",
    "\n",
    "1. Update the Alert Query\n",
    "2. Update the Time Window\n",
    "\n",
    "#### Update Alert Query\n",
    "\n",
    "Splunk can use time modifiers in the search query ([ref](https://docs.splunk.com/Documentation/SCS/current/Search/Timemodifiers)). To switch to `_indextime` alerting we need to add `_index_earliest` and `_index_latest` to our search query. These parameters define the `_indextime` range we intend to search. \n",
    "\n",
    "`_index_earliest` - In most cases, set this to the time interval the alert looks back on. (e.g. `_index_earliest=-15m`) \\\n",
    "\n",
    "`_index_latest` - In most cases, this should be set to the current time. (e.g. `_index_latest=now()`)\n",
    "\n",
    "#### Update Time Window\n",
    "\n",
    "Switching to `_indextime` alerting will not remove the use of `_time`; it is still a filter on the search. Instead, `_time` is now the maximum log delay that still generates an alert. \n",
    "\n",
    "#### An Example\n",
    "\n",
    "The below search is updated to alert on `_indextime` alerting. The figure is marked relative to the behavior referenced in the list:\n",
    "\n",
    "1. Search every 15 minutes\n",
    "2. Search everything indexed within the last 15 minutes \n",
    "3. Search everything with _time within the last 14 days*\n",
    "\n",
    "*This does not significantly impact processing; it is not the same as processing 14 days of data\n",
    "\n",
    "![](splunk_blog_2.png \"Fig 2. Example Updated Splunk Alert\") \n",
    "_Fig 2. Example Updated Splunk Alert_\n",
    "\n",
    "This alert behaves in the following manner:\n",
    "1. Runs every 15 minutes\n",
    "2. Searches logs indexed in the last 15 minutes\n",
    "3. Searches logs delayed up to 14 days\n",
    "\n",
    "Simply using `_time` can cause issues with saved searches, even in logs are delayed by minimal amounts. Switching to `_indextime` alerting reduces this problem significantly. In closing, I recommend running all saved searches with this method.*\n",
    "\n",
    "#### Additional Items\n",
    "\n",
    "This post did not address the following:\n",
    "\n",
    "* This does not address or identify issues that set `_time` in the future. These should be identified separately and remediated.\n",
    "* Certain saved searches may require additional effort to adopt this method. For example: searches with subsearches, searches with multiple time frames, searches in which `_time` is a part of calculations or needs to be precise.\n",
    "* Switching to `_indextime` alerting is not a substitute for fixing unacceptable logging delay."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dos-cazadores",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f59aee3d2390d529bce8bf606822e92cb5ef2be94c82524a3e8a4b50649d14a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
