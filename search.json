[
  {
    "objectID": "content/blog/posts/splunk/splunklogdelay.html",
    "href": "content/blog/posts/splunk/splunklogdelay.html",
    "title": "Log Timing Delays in Splunk Could be Putting Your Security at Risk",
    "section": "",
    "text": "BLUF: Logging delay to Splunk as little as 100ms can cause missed alerts.\n\nA Tale of Two Times\nSplunk is a powerful platform for collecting, indexing and analyzing data from various sources. However, it is critically important to understand the nuances of how Splunk handles time; forgoing such consideration may lead to missing critical events.\n\n_time\nThe _time field in Splunk represents the timestamp of an event as specified by the source log. This field is critical for time-based analysis, such as identifying trends, detecting anomalies, and correlating events across different sources. This is the time that Splunk visualizes when viewing search results and is used as the time variable when performing searches.\n\n\n_indextime\nThe _indextime field in Splunk represents the timestamp when Splunk indexes the event, which may not always be the same as the _time field. Simply, it’s the time Splunk logged the event, and it will almost always differ from _time even if by milliseconds.\n\n\n\nA Wrinkle in Time\nWhy do the times differ?\nTruth is, there is a delay. There are many points in the ingestion pipeline that can cause delay, such as:\n\nNetwork latency\nLogging throughput capacity\nLogging interval\nClock skew\n\nTiming delays in Splunk can cause missed alerts, which may result in serious consequences for security and other critical operations.\nFor instance, if an alert is monitoring a critical system, but there is a delay between the _time and _indextime timestamps, the alert may not be triggered even when events occur.\n\n\nSmall Delay = Big Problem\nHow much delay can cause missed events?\nDelays of milliseconds can result in missed events. During testing in the worst case scenario, delays of 100ms resulted in missed events. Theoretically the delay can be even be lower. This raises an important question - are you confident that all of your critical logging sources consistently log in near real-time?\n\n\nDelay Detection\nCan I calculate delays in my log sources?\nYes, Splunk has a great article on this: here.\nTL;DR To discover log delays subtract _time from _indextime.\nindex=example_delay | eval delay_sec=_indextime-_time | eval delay_minute=(delay_sec/60) | timechart span=1h avg(delay_minute) by index\n Fig 1. Example Timechart for Splunk Search\nCan I discover if any of my recent saved searches missed events?\nYes, a simple method is to re-run the search over the same period and compare the results. If the results differ over there is likely a timing issue.\nBelow is an example script in Python to identify saved searches with timing issues. Usage information: here.\nI’ve run this script against a lab instance of Splunk with intentionally induced delays, the results are below.\n\n\nCode\n#Code to detect and graph differences in recently ran searches\n\n# Import required libraries\nimport time\nimport json\nimport splunklib.client as client\nimport splunklib.results as results\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom datetime import datetime, timedelta\n\n\n# Function to connect to the Splunk service\ndef splunk_connect(base_url, port, splunk_account, splunk_password):\n\n    try:\n        # Connect to the Splunk service\n        service = client.connect(host=base_url, port=port, username=splunk_account, password=splunk_password)\n        print('Connected to Splunk instance: {}'.format(base_url))\n        print('-' * 90)\n    except Exception as e:\n        print('Error in Connecting to Splunk: {}'.format(e))\n    \n    # Return the service object\n    return service\n\n\n# Function to compare jobs\ndef compare_jobs(service,owner,graph):\n    \n    # Get the list of search jobs\n    search_jobs = service.jobs.list()\n    \n    # Loop through the search jobs\n    for job in search_jobs:\n        # If the job is a saved search and the owner matches the specified owner\n        if job['content']['isSavedSearch'] == '1' and job['access']['owner'] == owner:\n     \n            # Initialize lists to store results\n            results_list = []\n            results_list_new = []\n            \n            # Set the keyword arguments for the search job\n            kwargs =    {\"earliest_time\": job['content']['earliestTime'],\n                         \"latest_time\": job['content']['latestTime'],\n                         \"search_mode\": \"normal\",\n                         \"output_mode\": \"json\"}\n                \n            # Get the results of the original job\n            job_results = job.results(output_mode='json')\n            \n            # Parse the results and append them to the results list\n            for x in results.JSONResultsReader(job_results):\n                results_list.append(dict(x))\n                    \n            # Create a new job with the same search criteria\n            job_new = service.jobs.create(job['content']['eventSearch'],**kwargs)\n            \n            # Wait for the new job to finish\n            while not job_new.is_done():\n                time.sleep(1)\n                    \n            # Get the results of the new job\n            job_results_new = job_new.results(output_mode='json')\n            \n            # Parse the results and append them to the new results list\n            for y in results.JSONResultsReader(job_results_new):\n                results_list_new.append(dict(y))\n                \n            # Compare the lengths of the results lists\n            if(len(results_list) != len(results_list_new)):\n                # Initialize lists to store issues\n                search_issue = []\n                results_indexed = []\n                results_time = []\n                # Add the search issue to the list\n                search_issue.append({\n                    'sid': job['content']['sid'],\n                    'label': job['content']['label'],\n                    'earliest_time': job['content']['earliestTime'],\n                    'latest_time': job['content']['latestTime'],\n                })\n                    \n                # Get the unique keys for the results\n                results_keys = [(d.get('_raw'), d.get('_indextime'), d.get('_time')) for d in results_list]\n                results_keys_new = [(d.get('_raw'), d.get('_indextime'), d.get('_time')) for d in results_list_new]\n                \n                # Find the unique results in the new results list\n                unique_to_new = set(results_keys_new) - set(results_keys)\n\n                # print the unique results\n                if unique_to_new:\n                    print('SEARCH: {} SID: {}'.format(job['content']['label'],job['content']['sid']))\n                    print('RESULT: DIFFERENCE FOUND')\n                    print('NOTE: This is indicative of a delay issue and resulted in missing events in the original search.')\n                    print('-' * 10)\n                    print('List of Missing Results')\n                    print('-' * 10)\n                    \n                    #format time and set in index\n                    for key in unique_to_new:\n                        formatted_indextime = datetime.utcfromtimestamp(int(key[1])).strftime('%Y-%m-%d %H:%M:%S')\n                        results_indexed.append(formatted_indextime)\n                        formatted_time = datetime.fromisoformat(key[2]).strftime('%Y-%m-%d %H:%M:%S')\n                        results_time.append(formatted_time)\n                        print('        _time={},_indextime={},_raw={}'.format(formatted_time,formatted_indextime,key[0]))\n                    \n                    #graph the results    \n                    if graph == 'true':     \n                        plot_timeline(search_issue,results_indexed,results_time)\n                    print('-' * 90)\n            else:\n                print('SEARCH: {} SID: {}'.format(job['content']['label'],job['content']['sid']))\n                print('RESULT: NO DELAY FOUND')\n                print('NOTE: Search results in original search and re-ran search match, no delay issue identified.')\n                print('-' * 90)\n \n\n# Function to plot the timeline of the search issue\ndef plot_timeline(search_issue, results_indexed, results_time):\n    \n    # Convert result issue timestamps to datetime objects\n    indextimes = [datetime.strptime(d, '%Y-%m-%d %H:%M:%S') for d in results_indexed]\n    times = [datetime.strptime(d, '%Y-%m-%d %H:%M:%S') for d in results_time]\n    # Set the title and subtitle for the plot\n    title = search_issue[0]['label']\n    subtitle = search_issue[0]['sid']\n    \n    # Get the start and end dates of the search issue and remove timezone and microseconds\n    start_date = datetime.strptime(search_issue[0]['earliest_time'], '%Y-%m-%dT%H:%M:%S.%f%z')\n    start_date = start_date.replace(microsecond=0, tzinfo=None)\n    end_date = datetime.strptime(search_issue[0]['latest_time'], '%Y-%m-%dT%H:%M:%S.%f%z')\n    end_date = end_date.replace(microsecond=0, tzinfo=None)\n\n    # Create a figure and axis for the plot\n    fig, ax = plt.subplots(figsize=(12, 6))\n\n    # Plot the base timeline\n    ax.plot([start_date, end_date], [0, 0], color='black', alpha=0.5)\n    # Fill the area between the start and end dates\n    ax.fill_between([start_date, end_date], -0.5, 0.5, color='green', alpha=0.2)\n\n    # Calculate the maximum difference between indextime and start_time\n    max_difference = max([abs((date - start_date).total_seconds()) for date in indextimes] +\n                         [abs((date - end_date).total_seconds()) for date in indextimes])\n    \n    # Set the buffer based on the maximum difference\n    buffer = timedelta(seconds=max_difference * 0.1)\n\n    # Set the x and y limits for the plot\n    ax.set_xlim(start_date - buffer, end_date + buffer)\n    ax.set_ylim(-0.5, 0.5)\n\n    # Remove spines and y-axis\n    ax.spines['left'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['top'].set_visible(False)\n    ax.yaxis.set_visible(False)\n\n    # Plot the indexed markers and annotations\n    for idx, date in enumerate(indextimes):\n        ax.plot(date, 0, 'ro', markersize=8, label='Indexed Time for Event')\n        ax.annotate('{} - {}'.format(idx,date.strftime('%m-%d-%Y %H:%M:%S')), xy=(date, 0), xytext=(date, 0.1 + (idx % 2) * 0.1),\n                arrowprops=dict(arrowstyle='-&gt;', lw=1.5),\n                fontsize=12, ha='center')\n        \n    #plot the time markers and annotations\n    for idx, date in enumerate(times):\n        ax.plot(date, 0, 'bo', markersize=8, label='Time for Event')\n        ax.annotate('{} - {}'.format(idx,date.strftime('%m-%d-%Y %H:%M:%S')), xy=(date, 0), xytext=(date, -0.1 + (idx % 2) * -0.1),\n                arrowprops=dict(arrowstyle='-&gt;', lw=1.5),\n                fontsize=12, ha='center')\n\n    # Set the title, x-axis formatter, and x-axis locator\n    ax.set_title('{}\\n{}'.format(title,subtitle), fontsize=16, fontweight='bold')\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d-%Y %H:%M:%S'))\n    ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n\n    # Create the legend and display unique items\n    handles, labels = ax.get_legend_handles_labels()\n    by_label = dict(zip(labels, handles))\n    ax.legend(by_label.values(), by_label.keys(), loc='upper left')\n    \n    # Adjust layout and display the plot\n    plt.tight_layout()\n    plt.show()\n\n\ndef banner():\n    print('-' * 90)\n    print('-' * 90)\n    print(' _____  _____       _____  _   _   _____  _____ _____ _____ _____ _____ _____ _    _ _____')\n    print(' |    \\ |____ |     |___|   \\_/    |    \\ |____   |   |____ |       |     |    \\  /  |____')\n    print(' |____/ |____ |____ |   |    |     |____/ |____   |   |____ |_____  |   __|__   \\/   |____')\n    print('                                 Splunk Log Delay Detector                            v1.0')\n    print('-' * 90)\n    print('-' * 90)\n    \n    \ndef main():\n    \n    #get config\n    with open('config.json') as f:\n        data = json.load(f)\n        splunk_account = data['splunk_account']\n        splunk_password = data ['splunk_password']\n        base_url = data['base_url']\n        port = data['port']\n        owner = data['owner']\n        graph = data['graph']\n  \n    #show banner\n    banner()\n  \n    #connect to splunk\n    service = splunk_connect(base_url, port, splunk_account, splunk_password)\n    \n    #check and compare all jobs by owner\n    compare_jobs(service,owner,graph) \n    \n    \nif __name__ == \"__main__\":\n    main()\n\n\n------------------------------------------------------------------------------------------\n------------------------------------------------------------------------------------------\n _____  _____       _____  _   _   _____  _____ _____ _____ _____ _____ _____ _    _ _____\n |    \\ |____ |     |___|   \\_/    |    \\ |____   |   |____ |       |     |    \\  /  |____\n |____/ |____ |____ |   |    |     |____/ |____   |   |____ |_____  |   __|__   \\/   |____\n                                 Splunk Log Delay Detector                            v1.0\n------------------------------------------------------------------------------------------\n------------------------------------------------------------------------------------------\nConnected to Splunk instance: 192.168.42.138\n------------------------------------------------------------------------------------------\nSEARCH: indextime_demo SID: scheduler__splunk__search__RMD51aafb4f9eda25a49_at_1680733200_806\nRESULT: DIFFERENCE FOUND\nNOTE: This is indicative of a delay issue and resulted in missing events in the original search.\n----------\nList of Missing Results\n----------\n        _time=2023-04-05 22:19:59,_indextime=2023-04-05 22:20:02,_raw=critical ransomware L0OdbQcw6y\n        _time=2023-04-05 22:19:57,_indextime=2023-04-05 22:20:00,_raw=critical ransomware 3X1K0CTmxE\n------------------------------------------------------------------------------------------\nSEARCH: indextime_demo SID: scheduler__splunk__search__RMD51aafb4f9eda25a49_at_1680566280_768\nRESULT: NO DELAY FOUND\nNOTE: Search results in original search and re-ran search match, no delay issue identified.\n------------------------------------------------------------------------------------------\n\n\n\n\n\n\n\nA Solution\nRather than telling Splunk to alert to items within the last _time window for our searches, we can alert to anything indexed in the period since our search last ran. Instead saying, “Splunk tell me about any log you just discovered.”\nTo do this the alert requires two modifications:\n\nUpdate the Alert Query\nUpdate the Time Window\n\n\nUpdate Alert Query\nSplunk can use time modifiers in the search query (ref). To switch to _indextime alerting we need to add _index_earliest and _index_latest to our search query. These parameters define the _indextime range we intend to search.\n_index_earliest - In most cases, set this to the time interval the alert looks back on. (e.g. _index_earliest=-15m)\n\n_index_latest - In most cases, this should be set to the current time. (e.g. _index_latest=now())\n\n\nUpdate Time Window\nSwitching to _indextime alerting will not remove the use of _time; it is still a filter on the search. Instead, _time is now the maximum log delay that still generates an alert.\n\n\nAn Example\nThe below search is updated to alert on _indextime alerting. The figure is marked relative to the behavior referenced in the list:\n\nSearch every 15 minutes\nSearch everything indexed within the last 15 minutes\nSearch everything with _time within the last 14 days*\n\n*This does not significantly impact processing; it is not the same as processing 14 days of data\n Fig 2. Example Updated Splunk Alert\nThis alert behaves in the following manner: 1. Runs every 15 minutes 2. Searches logs indexed in the last 15 minutes 3. Searches logs delayed up to 14 days\nSimply using _time can cause issues with saved searches, even in logs are delayed by minimal amounts. Switching to _indextime alerting reduces this problem significantly. In closing, I recommend running all saved searches with this method.*\n\n\nAdditional Items\nThis post did not address the following:\n\nThis does not address or identify issues that set _time in the future. These should be identified separately and remediated.\nCertain saved searches may require additional effort to adopt this method. For example: searches with subsearches, searches with multiple time frames, searches in which _time is a part of calculations or needs to be precise.\nSwitching to _indextime alerting is not a substitute for fixing unacceptable logging delay."
  },
  {
    "objectID": "content/blog/blog.html",
    "href": "content/blog/blog.html",
    "title": "Blog Direcotry",
    "section": "",
    "text": "Log Timing Delays in Splunk Could be Putting Your Security at Risk\n\n\n\n\n\nDiscover how minor logging delays can lead to missed Splunk alerts and put your security at risk. Learn how to avoid this issue.\n\n\n\n\n\n\nApr 19, 2023\n\n\nMichael McKinley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blue-Prints Blog",
    "section": "",
    "text": "@McKinleyMike @TheLatteri\n\n\nMichael McKinley is a Lead Security Engineer in the financial services sector. He is responsible for detecting, disrupting, profiling, and disclosing attacks by APTs targeting the financial industry, such as the attempt by FIN7 to spread malware through the spoofing of the SEC EDGAR filing system. Additionally, Michael has discovered and disclosed security vulnerabilities in leading enterprise software.\nWhen not hacking, Michael enjoys spending time in the great outdoors.\n\n\nTony Latteri is a seasoned blue team operator and Principal Security Engineer in the financial services sector with a passion to deny, degrade, and disrupt cyber adversary operations.\nWhen not bringing pain to the adversary, Tony enjoys picking up and putting down heavy things."
  },
  {
    "objectID": "index.html#blog-posts",
    "href": "index.html#blog-posts",
    "title": "Blue-Prints Blog",
    "section": "Blog Posts",
    "text": "Blog Posts\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nLog Timing Delays in Splunk Could be Putting Your Security at Risk\n\n\n4/19/23\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#subscribe",
    "href": "index.html#subscribe",
    "title": "Blue-Prints Blog",
    "section": "Subscribe",
    "text": "Subscribe\nSubscribe via  RSS to recieve updates."
  }
]